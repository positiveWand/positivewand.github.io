---
layout: post
title: "비트(bit)-정보 표현의 최소 단위"
toc: true
---

**비트(bit)** 는 2진법을 사용한다는 뜻의 **Binary** 와 숫자를 뜻하는 **Digit** 의 합성어이다. 정보를 제공할 수 있는 데이터를 표현하는 최소 단위이며 컴퓨터는 비트를 이용해 표현한 데이터를 저장 및 처리한다. 따라서 컴퓨터를 이해하기 위한 첫걸음으로 이번 포스트에서는 비트를 이용해 정보를 표현하는 기본 원리와 그것의 필요성 그리고 비트와 관련된 용어들을 알아보도록하자.

# 비트(bit)란?
**비트(bit)** 는 0 또는 1의 값을 갖는 정보 제공을 위한 최소 단위이다. 컴퓨터에서는 비트를 이용해 정보 또는 데이터를 표현한다.

## 비트는 정보를 제공한다
**정보(information)** 는 불확실한 것을 확실하게 알도록 해주는 것이다. 애매하고 분간이 되지 않던 것들을 명확하게 구별하기 위해서는 정보가 필요하다. 비트는 이런 구별, 다시 말해 정보 제공을 위한 최소 단위이며 개념이나 물체를 서로 다른 2개로 구별할 수 있는만큼의 정보를 제공한다. 하나 0이라면 다른 하나는 1인 것이다. 1개의 비트로는 세상을 두 부분으로만 나누고 구별할 수 있는 것이다.

## 비트를 이용한 정보 표현
1개의 비트로는 2개의 개념 또는 물체 밖에 표현하지 못한다. 컴퓨터에서는 이보다 훨씬 더 많은 것을 표현하기를 원하기 때문에 이를 위해 여러 개의 비트를 사용한다.

1개의 비트는 기존의 것을 서로 다른 2개로 구분할 수 있으므로 n개의 비트를 이용한다면 2<sup>n</sup>개를 구분할 수 있다. 한 개의 비트가 추가될 때마다 기존의 구분보다 2배 더 많이 구분할 수 있게되는 것이다. 10개의 서로 다른 것들을 표현하고 싶다면 최소 4개의 비트가 필요하다는 것이고 이는 2<sup>3</sup>=8 보다는 많이, 최소 2<sup>3</sup>=16만큼 표현가능해야하기 때문이다.

## 인코딩(encoding), 디코딩(decoding) 그리고 문맥(context)
어떤 정보의 형태나 형식을 변환하는 처리 혹은 처리 방식을 **인코딩(encoding)** 이라고 한다고 한다. 그리고 인코딩을 통해 변환한 정보를 원래의 형태나 형식으로 복원하는 것을 **디코딩(decoding)** 이라고 한다. 다양한 경우에 사용되는 용어지만 여기에서는 임의의 정보를 비트 조합으로 표현하는 것이 인코딩, 비트 조합으로부터 원래 정보를 복원하는 것이 디코딩이다.

컴퓨터는 비트라는 기호 단위를 이용해 숫자, 문자를 표현한다. 숫자, 문자를 통해 표현된 정보를 비트라는 형식으로 변환하여 사용한다. 충분한 개수의 비트가 있다면 실수, 정수, 한글, 알파벳 등을 비트를 이용해 표현할 수 있을 것이다. 각 비트 조합에 대응하는 개념, 물체를 지정하기만 하면된다. 그 대응관계와 변환 방법을 정의한 것이 인코딩-디코딩 방식이며 사용하는 인코딩-디코딩 방식, 즉 **문맥(context)**에 따라 같은 비트 조합이 다르게 해석될 수 있다. 1011이라는 비트 조합이 해석되는 문맥에 따라 문자 "K"를 나타낼 수도 있고 숫자 "11"를 나타낼 수도 있는 것이다.

> 컴퓨터는 **비트(bit)**라는 최소단위를 이용해 정보를 **인코딩**한다. 많은 비트를 사용할수록 더 많은 정보를 표현할 수 있다. **문맥** 또는 **인코딩-디코딩 방식**에 따라 같은 비트 조합을 다르게 해석할 수 있다.

# 비트를 사용하는 이유
컴퓨터가 비트를 사용하는 이유는 아날로그보다는 디지털이 10진수보다는 2진수가 컴퓨터의 역할을 수행하는데 여러모로 유리하기 때문이다.

## 아날로그보다는 디지털
우주는 인간이 통제할 수 없는 잡음이 필연적으로 발생한다. 특히 컴퓨터를 작게 만들면 부품 간의 간섭이 심해지기 때문에 잡음에 영향을 받지 않는 설계가 더욱 더 중요해진다.

아날로그 하드웨어 장치는 연속적인 상태와 성질을 이용하기에 잡음에 민감하고 이에 따라 측정된 값의 정확성을 보장하지 못하는 매우 불안정한 장치이다. 이를 보완하기 위해 잡음 저항성을 향상시키면(거의 불가능하지만) 장치는 복잡해지고 생산 비용은 매우 비싸진다.

반면에 디지털 하드웨어 장치는 이산적인 상태와 성질을 이용하기에 잡음으로부터 영향을 쉽게 받지 않고 이에 따라 측정된 값이 아날로그 장치에 비해 훨씬 안정적이다. 이는 디지털 장치는 아날로그 장치와는 다르게 작은 값의 변화로는 값이 바꾸지 않고 값이 바뀌기 위해서는 **임계값(threshold)** 이상의 변화가 필요하기 때문이다

## 10진수보다는 2진수
정보 표현을 위해 반드시 2진수가 사용될 필요는 없을 것이다. 하지만 우리에게 익숙한 10진수가 사용되지 않고 2진수가 사용되는 이유는 무엇일까.

이는 아날로그 장치보다 디지털 장치가 선호되는 이유와 비슷하다. 10진수는 한 개의 단위가 10개의 상태를 표현하는 반면 2진수는 한 개의 단위가 2개의 상태만을 표현하기 때문에 2진수가 상태끼리의 구분하기 위한 판단 기준이 10진수에 비해 훨씬 간단하고 이는 하드웨어의 단순화, 비용 절감으로 이어지기 때문이다.

> 비트는 이산적이고 표현 가능한 상태가 적기 때문에 **잡음 저항이 높고 안정적**이다. 이는 하드웨어 설계를 단순화시키고 생산 비용을 크게 절감하기 때문에 **비트는 효율적인 정보 표현 수단으로써 컴퓨터 내에서 사용된다**.

# 비트 다루기
## 특수한 비트
정보를 표현한 비트 조합 중 가장 오른쪽에 있는 비트는 **가장 작은 유효 비트(least significant bit, LSB)**라고 부르고, 가장 왼쪽의 비트를 **가장 큰 유효 비트(most significant bit, LSB)**라고 부른다. LSB가 변할 경우 2진수 값(= 비트 조합)이 가장 적게 변하고 MSB가 변할 경우 2진수 값이 가장 많이 변하기 때문에 그렇게 칭한다.

> **가장 작은 유효 비트(least significant bit, LSB)**: 가장 오른쪽에 있는 비트

> **가장 큰 유효 비트(most significant bit, MSB)**: 가장 왼쪽에 있는 비트

## 비트 조합 쉽게 표현하기
비트의 나열은 0과 1의 나열인 2진수이다. 이런 형태는 표현하는 정보가 많아질수록 길이가 점점 길어지고 시각적으로도 사용성 측면에서도 불편하다. 따라서 이를 보완하기 위해 **8진 표현법**과 **16진 표현법**을 사용하여 2진수를 표현할 수 있다.

### 8진 표현법(Octal representation)
2진수의 비트들을 3개씩 묶어 2진수를 8진수로 표현한다면 길이가 훨씬 짧아져 데이터를 읽기가 편해진다. 그렇게 데이터를 8진수로 표현하는 방법을 **8진 표현법(Octal representation)** 이라고 한다. 8진수는 밑이 8인 수이기 때문에 각 자릿수에는 0~7의 숫자가 올 수 있다.

보통 8진 표현법을 통해 표현된 숫자를 다른 진수의 숫자(보통 10진수)와 구별하기 위해 8진수 앞에는 0을 붙여준다.

예를 들어, 숫자 017은 8진수이며 값은 10진수로 8+7=15이다.

> **8진 표현법**은 2진수를 **3개씩** 묶어 8진수로 표현한다.

### 16진 표현법(Hexadecimal representation)
묶음의 크기를 늘려 2진수의 비트들을 4개씩 묶어 2진수로 16진수로 표현하여 더욱 짧게 데이터를 표현할 수 있다. 그렇게 16진수로 데이터를 표현하는 방법을 **16진 표현법(Hexadecimal representation)** 이라고 한다. 16진수는 밑이 16인 수이기 떄문에 0~9의 숫자로는 자릿수를 완전히 표현하지 못하며 이를 위해 A=10, B=11, C=12, D=13, E=14, F=15로 정의하여 A~F의 알파벳을 추가로 사용한다.

보통 16진 표현법을 통해 표현된 숫자는 다른 진수의 숫자와 구별하기 위해 16진수 앞에는 0x를 붙여준다.

예를 들어, 숫자 0x13D는 16진수이며 값은 10진수로 256+48+13=317이다.

> **16진 표현법**은 2진수를 **4개씩** 묶어 16진수로 표현한다.

8진 표현법과 16진 표현법은 단순히 비트를 묶어서 표현해주기만하면 적절한 숫자가 나오는데 이는 8과 16이 모두 2의 거듭제곱이기 때문이다.

## 비트 묶음 단위
비트의 개수만으로 데이터의 크기를 세는 것은 비효율적이기 때문에 비트 묶음들에 대한 정의가 필요하다. 과거에는 다양한 크기의 비트 묶음을 사용했지만 현재 널리 사용되고 인정받는 단위는 바이트이다.

### 바이트(Byte)
**바이트(Byte)**는 8 bit 묶음을 칭하는 단위이다. 1 byte = 8 bit인 것이다. 현재 컴퓨터 내에서는 데이터의 저장과 처리에 보통 바이트를 기본 단위로 혹은 그것의 배수만큼의 크기를 기본 단위로 사용한다.

### 큰 바이트 단위
대용량의 바이트를 표현하기 위해서는 미터법에서의 접두사들을 가져와 사용한다. **킬로(kilo)**, **메가(mega)**, **기가(giga)**, **테라(tera)** 등이 그것이며 이에 따라 **킬로바이트(KB)**, **메가바이트(MB)**, **기가바이트(GB)**, **테라바이트(TB)**가 되는 것이다.

하지만 미터법으로부터 용어를 가져와서인지 킬로바이트, 메가바이트, 기가바이트는 크기가 컴퓨터 엔지니어들에게 익숙한 2의 거듭제곱으로 사용되기도 하지만 대중적인 10의 거듭제곱으로도 사용되기도한다. 이와 관련된 시비가 있어 2의 거듭제곱 크기들을 위한 새로운 표준 접두사가 생겼고 **키비(KiB)**, **메비(MiB)**, **기비(GiB)**, **테비(TiB)**가 그것이다.

<table>
<thead>
  <tr>
    <th colspan="3">SI 접두사</th>
    <th colspan="2">이진 접두사</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>단위 이름</td>
    <td>10의 거듭제곱 값</td>
    <td>2의 거듭제곱 값</td>
    <td>단위 이름</td>
    <td>값</td>
  </tr>
  <tr>
    <td>킬로바이트(KB)</td>
    <td>10<sup>3</sup></td>
    <td>2<sup>10</sup></td>
    <td>키비바이트(KiB)</td>
    <td>2<sup>10</sup></td>
  </tr>
  <tr>
    <td>메가바이트(MB)</td>
    <td>10<sup>6</sup></td>
    <td>2<sup>20</sup></td>
    <td>메비바이트(MiB)</td>
    <td>2<sup>20</sup></td>
  </tr>
  <tr>
    <td>기가바이트(GB)</td>
    <td>10<sup>9</sup></td>
    <td>2<sup>30</sup></td>
    <td>기비바이트(GiB)</td>
    <td>2<sup>30</sup></td>
  </tr>
  <tr>
    <td class="tg-0lax">테라바이트(TB)</td>
    <td class="tg-0lax">10<sup>12</sup></td>
    <td class="tg-0lax">2<sup>40</sup></td>
    <td class="tg-0lax">테비바이트(TiB)</td>
    <td class="tg-0lax">2<sup>40</sup></td>
  </tr>
</tbody>
</table>

# 참고
- 조너선 스타인하트, 한 권으로 읽는 컴퓨터 구조와 프로그래밍, 책만, 2021